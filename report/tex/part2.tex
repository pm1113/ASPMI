\section{Parametric and Line Spectra}

\subsection{Correlation Estimate}

\noindent{}a. Figure \ref{fig:correlogram} shows the biased and unbiased estimates of the autocorrelation function (ACF) as well as the correlogram spectral estimates obtained for 3 signals: white gaussian noise, a sinewave and filtered white gaussian noise.\\

\noindent{}It is clear that the ACF estimates are similar at small lags; however the similarity fades as the lag increases. The biased estimator function imposes a Bartlett window on the ideal ACF whereas the unbiased estimator imposes a rectangular window on the ideal ACF. The choice of window is extremely important and the devastating effects of using a wrong window can be understood by studying the correlograms. The spectral estimate obtained from the unbiased ACF estimate returns values of power that are negative; this is obviously not possible. \\

\noindent{}This problem can also be understood in terms of the time-domain windowing that is performed prior to computing the spectral estimate. The biased estimator of ACF is obtained by windowing the time-domian signal using rectangular window. When we calculate the periodogram using the time-domain signal, the magnitude spectrum of the window is squared; a rectangular window ensures positive semidefiniteness of the resulting power spectrum. The same cannot be said about the window that is applied to the time-domain signal to obtain an unbiased estimate of ACF; as observed, the window does not preserve positive semidefiniteness of the spectral estimate.

\begin{figure}[H]
\centering{}
\includegraphics[width=0.32\textwidth]{part2/wgn_biased_and_unbiased_estimates}
\includegraphics[width=0.32\textwidth]{part2/sinewave_biased_and_unbiased_estimates}
\includegraphics[width=0.32\textwidth]{part2/filt_wgn_biased_and_unbiased_estimates}
\includegraphics[width=0.32\textwidth]{part2/wgn_correlogram}
\includegraphics[width=0.32\textwidth]{part2/sinewave_correlogram}
\includegraphics[width=0.32\textwidth]{part2/filt_wgn_correlogram}
\caption{Biased and Unbiased Estimates of ACF and their Effect on the Correlogram}
\label{fig:correlogram}
\end{figure}

\noindent{}b. The figures below show 100 realizations of 2 unique random processes. The mean of the 100 realizations has been graphed in red. It is clear from observing the plots that the each random process consists of 2 sinewaves.

\begin{figure}[H]
\centering{}
\includegraphics[width=0.32\textwidth]{part2/variance_correlogram_sine_waves_4_8}
\includegraphics[width=0.32\textwidth]{part2/variance_correlogram_sine_waves_1_2}
\end{figure}

\begin{figure}[H]
\centering{}
\includegraphics[width=0.32\textwidth]{part2/std_dev_correlogram_sine_waves_4_8}
\includegraphics[width=0.32\textwidth]{part2/std_dev_correlogram_sine_waves_1_2}
\caption{Variance and Standard Deviation of Correlogram Spectral Estimates}
\end{figure}

\noindent{}The figure above also includes the standard deviation of the 100 spectral estimates. Notice that there are peaks in the standard deviation at exactly the same frequencies as the peaks in the periodogram estimates. This is indicative of one of the short-comings of the periodogram spectral estimator. \textbf{The periodogram is not a consistent estimator.} For the periodogram to be consistent estimator, $\text{var}(P_{per}) \rightarrow 0  \ \text{as} \ N \rightarrow \infty$. The peridogram does not satisfy that condition but instead,  $\text{var}(P_{per}) \rightarrow P_{x}^{2}(f) \ \text{as} \ N \rightarrow \infty$. This is clearly depicted in the plots of standard deviation below; it spikes at $f=0.4, 0.8$ Hz and $f=1.2, 2.4$ Hz. Thus, simply collecting more samples and then calculating the periodogram will not reduce variance. Other spectral estimation techniques such as the Blackman-Tukey method must be considered if variance is to be reduced.\\

\noindent{}c. Notice that when the graphs are plotted in the dB, the effects described above are made even more obvious. It is interesting to note that the effects of windowing the sinewave are now also clear. Notice the $sinc^2$-like shape near the peaks at $f=1.2, 2.4$ Hz.
 
\begin{figure}[H]
\centering{}
\includegraphics[width=0.32\textwidth]{part2/variance_correlogram_sine_waves_1_2_dB}
\includegraphics[width=0.32\textwidth]{part2/std_dev_correlogram_sine_waves_1_2_dB}
\caption{Effect of dB Scale on Spread of Variance and Standard Deviation of Correlogram Spectral Estimates}
\end{figure}

\noindent{}d. Since the frequency resolution of the periodogram is proportional to $\frac{1}{N}$, we expect to be able to distinguish 2 complex exponentials with frequencies of $0.3$ Hz and $0.32$ Hz with approximately $50$ samples; with $50$ samples, we will have a resolution of approximately $0.02$ Hz. The figures below confirm this. As the number of samples increases, the ability to distinguish the peaks increases. Note that the periodograms are no longer symmetric. This is expected since the functions are complex. Due to their non-symmetric nature, the one-sided periodogram is plotted up to $f_s$ instead of $\frac{f_s}{2}$.

\begin{figure}[H]
\centering{}
\includegraphics[width=0.32\textwidth]{part2/complex_psd_n_30}
\includegraphics[width=0.32\textwidth]{part2/complex_psd_n_50}
\includegraphics[width=0.32\textwidth]{part2/complex_psd_n_70}
\caption{Effect of Increasing Number of Samples on the Resolution of Periodogram Spectral Estimates}
\label{fig:complex_periodogram}
\end{figure}

\newpage
\noindent{}e. The MUltiple SIgnal Classification method (MUSIC) is a frequency estimation technique that models the ACF matrix $\textbf{R}_{xx}$ as the sum of 2 ACF matrices, namely the signal ACF matrix $\textbf{R}_s$ and the noise ACF matrix $\textbf{R}_n$. A specific scenario in which the signal only contains 1 complex exponential will have the following form:

\begin{equation}
x(n) = A_1 e^{jn\omega_1} + w(n)
\end{equation}

\noindent{}Since the matrix $\textbf{R}_{xx}$ is Hermitian, the eigenvectors of the matrix are orthogonal. As such, the following equation is true, where $\textbf{e}_{i}$ is a eigenvector from matrix $\textbf{R}_s$ whereas $\textbf{v}$ is an eigenvector from $\textbf{R}_n$ and $\textbf{R}_{xx} \in \mathbb{R}^{M \times M}$ and there are $p$ complex exponentials present in the signal:

\begin{equation}
\textbf{e}_{i}^H \textbf{v} = \sum_{k=0}^{M-1} v(k)e^{-jk\omega_{i}} = 0, \ i=1,2,\dots,p
\end{equation}

\noindent{}The power spectrum can be estimated using the following formula, where $\textbf{e} = [1, e^{j\omega}, e^{j2\omega},\dots, e^{j(M-1)\omega}]$. Note that a discrete-time signal can contain only a finite set of unique complex exponentials that are all contained within the vector $\textbf{e}$. This is due to the fact that the complex exponentials are periodic and this forms the basis of the Discrete Fourier Transform:

\begin{equation}
\hat{P}_{MU}(e^{j\omega}) = \frac{1}{\sum_{i=p+1}^M|\textbf{e}^H v_{i}|^2}
\end{equation}

\noindent{}Since the eigenvectors $\textbf{e}$ and $v_{i}$ are orthogonal, the inner product should have $p$ roots which lie on the unit circle and correspond to the frequencies of the complex exponentials that are present in the signal. However, since $\textbf{R}_{xx} \in \mathbb{R}^{M \times M}$ where $M > p + 1$, then the inner product will have $M-p-1$ zeros which lie anywhere on the z-plane. In fact, the zeroes could lie very close to the unit circle and thus lead to suprious peaks in the power spectrum that could be mistaken as a complex exponentials that is present within the signal. To reduce the likelihood of this occuring, we average the estimate using all of the $M-p-1$ eigenvectors corresponding to the noise. The averaging effect will amplify the magnitude of the true peaks and thus makes identifying complex exponentials that are actually present within our signal easy. \\

\noindent{}Having understood how the algorithm works, it is clear that the first line gives the ACF matrix $\textbf{R}_{xx} \in \mathbb{R}^{M \times M}$, where $M=14$. This value of $M$ is selected based on the fact that we can only trust that many ACF values. Larger values of $M$ could be selected however, as the lag increases, the accuracy with which ACF estimation can be performed decreases; poor ACF estimation at large lags has been illustrated above. The second line of the code performs the spectral estimation using the MUSIC algorithm. The ACF matrix is provided as the first argument while the dimension of the signal subspace $p$, is given as the second argument. The third and fourth arguments are related to the length of the FFT and the sampling period while the last argument informs the function to treat the first argument as a correlation matrix rather than a data matrix. 

\begin{figure}[H]
\centering{}
\includegraphics[width=0.32\textwidth]{part2/music_2_sinewaves_n_30}
\includegraphics[width=0.32\textwidth]{part2/music_3_sine_waves_n_30}
\includegraphics[width=0.32\textwidth]{part2/music_3_sine_waves_n_70}
\includegraphics[width=0.32\textwidth]{part2/music_2_sinewaves_n_30_overlay}
\includegraphics[width=0.32\textwidth]{part2/music_3_sinewaves_n_30_overlay}
\includegraphics[width=0.32\textwidth]{part2/music_3_sinewaves_n_70_overlay}
\caption{Detecting Sinewaves using MUSIC, and Effect of Increasing Number of Samples on the Resolution of Estimate}
\label{fig:music}
\end{figure}

\noindent{}The figures above shows the results obtained. The first row shows just one realization of the random signal $x$, whereas the second row shows an overlay of 100 realizations. It is clear that the MUSIC algorithm, like the periodogram has a frequency resolution limit as it is not able to identify 3 sinewaves that are very closely spaced together. However increasing the number of samples, like for the periodogram, increases the frequency resolution and distinguishing the sinewaves is now possible.\\

\noindent{}The main advantage of the MUSIC algorithm is the ability to identify complex exponentials when very few samples are available. This is clear when comparing the results obtained in Figure \ref{fig:music} to those obtained in Figure \ref{fig:complex_periodogram}. With just 30 samples, the MUSIC algorithm is able to correctly identify the location of the two peaks, while the periodogram is not.\\

\noindent{}The disadvantages are that the pseudospectrum does not provide information about the magnitude of the complex exponentials. Notice that the signal $x$ was generated with equal magnitudes for each of the complex exponentials but the pseudospectrum did not highlight this. Secondly, the algorithm requires, as an input, the dimension of the signal subspace. Often, this may not be known. Lastly, the averaging that is performed to remove spurious peaks from the spectra have the adverse effect of removing information about the noise spectra; this information may be useful, especially if the noise is non-white.

\newpage
\subsection{Spectrum of Autoregrssive Processes}

\noindent{}a. Autoregressive (AR) parameter estimation is performed using the following equation:

\begin{equation*}
\textbf{r}_{xx} = \textbf{R}_{xx}a \implies a = \textbf{R}_{xx}^{-1}\textbf{r}_{xx}
\end{equation*}

\noindent{}It is crucial that the matrix $\textbf{R}_{xx}$ be invertible. When the ACF is calculated using the biased estimator, $\textbf{R}_{xx}$ is guaranteed to be positive definite and thus invertible, however this is not gauranteed when the unbiased estimator is utilized.\\

\noindent{}b. Figure \ref{fig:AR_1000} shows the spectrum obtained when the signal $x(n)$ is modelled as a 4th, 8th, 10th and 14th order AR process. The effects of increasing the model order from $4$ to $8$ are clear. The 4th order process is not able to model the 2 distinct spikes that are present in the ideal spectrum, whereas the 8th order process is. Arbitrarily increasing the model order beyond 8 allows more degrees of freedom since there is greater flexibility to place poles in the spectrum. However, the resulting spectrum is not more accurate. In fact, increasing the number of poles causes both the peaks to approach the same magnitude; this is obviously not correct. This highlights the fact that the 500 samples used for estimation may not contain enough information about the AR process for effective parameter estimation.

\begin{figure}[H]
\centering{}
\includegraphics[width=0.24\textwidth]{part2/ar_spectrum_estimate_order_4}
\includegraphics[width=0.24\textwidth]{part2/ar_spectrum_estimate_order_8}
\includegraphics[width=0.24\textwidth]{part2/ar_spectrum_estimate_order_10}
\includegraphics[width=0.24\textwidth]{part2/ar_spectrum_estimate_order_14}
\caption{Power Spectrum Estimates Using All-Pole Autoregressive Model of Orders 4, 8, 10, 14}
\label{fig:AR_1000}
\end{figure}

\noindent{}c. Figure \ref{fig:AR_10000} shows that by increasing the number of samples, we obtain a much more accurate spectrum estimate. The conjecture above about 500 samples not containing enough information about the AR process is correct. Notice that with 10000 samples, a 4th order estimate is able to replicate the two spikes observed in the data. Increasing the model order to 12 does not cause both peaks to approach the same magnitude. \\

\noindent{}However, by setting the model order to 12, overfitting of the training data starts to occur. The peaks of the estimated spectrum have overshot the peak of the ideal spectrum. The power of the signal has been compressed into a small frequency range and thus made the signal highly periodic. However, by increasing the model order, the estimation error has been reduced. To balance the trade-off between generalization error and estimation error, measures such as the Akaike Information Criterion (AIC) or the Minimum Description Length (MDL) can be used to simultaneously penalize high model orders and estimation errors. 

\begin{figure}[H]
\centering{}
\includegraphics[width=0.32\textwidth]{part2/ar_spectrum_estimate_order_2_9500}
\includegraphics[width=0.32\textwidth]{part2/ar_spectrum_estimate_order_4_9500}
\includegraphics[width=0.32\textwidth]{part2/ar_spectrum_estimate_order_12_9500}
\caption{Effects of Underfitting and Overfitting Autoregressive Models}
\label{fig:AR_10000}
\end{figure}

\newpage
\subsection{Principal Component Analysis}

\noindent{}a. By studying the graphs below, it is clear that the training data $\textbf{X}$ has a rank of 3; $\textbf{X}$ only has 3 non-zero singular values. Notice that the matrix $\textbf{X}_{\text{noise}}$ also only has 3 dominant singular values. The magnitude of the singular values that come about from the effects of noise have roughly half the magnitude of the original dominant singular values. Note that the noise has not only introduced additional non-zero singular values, but it has only increased the magnitude of the original singular values that are due to the signal.

\begin{figure}[H]
\centering{}
\includegraphics[width=0.32\textwidth]{part2/svd_x}
\includegraphics[width=0.32\textwidth]{part2/svd_x_noise}
\caption{Singular Values of Matrices $\textbf{X}$ and $\textbf{X}_{\text{noise}}$}
\end{figure}

\noindent{}Next notice the squared error between each singular value of $\textbf{X}$ and $\textbf{X}_{\text{noise}}$. It is clear that the noise has slightly offset the original 3 singular values but the effects the noise has on the previously zero singular values is much more pronounced. It would be difficult to identify the rank of $\textbf{X}_{\text{noise}}$ if the noise had increased the previously zero singular values to reach similar magnitudes as the original singular values.

\begin{figure}[H]
\centering{}
\includegraphics[width=0.32\textwidth]{part2/squared_error_svd}
\caption{Squared Error between Each Singular Value of $\textbf{X}$ and $\textbf{X}_{\text{noise}}$}
\end{figure}

\noindent{}b. The basic low rank approximation $\tilde{\textbf{X}}_{\text{noise}}$ problem can be formulated as the following optimization problem:

\begin{align*}
\begin{aligned}
& \underset{\tilde{\textbf{X}}_{\text{noise}}}{\text{minimize}}
& & ||\textbf{X}_{\text{noise}}-\tilde{\textbf{X}}_{\text{noise}}||_{\text{F}}\\
& \text{subject to}
& & \text{rank} \big(\tilde{\textbf{X}}_{\text{noise}}\big) \leq r
\end{aligned}
\end{align*}

\noindent{}The above optimization problem admits as closed form solution in accordance with the Eckart-–Young–-Mirsky theorem; the theorem tells us to compute the approximation using only the $r$ most significant principal components. The theorem also tells us that the Frobenius norm squared of $\textbf{X}_{\text{noise}}-\tilde{\textbf{X}}_{\text{noise}, r}$ is equal to the squared sums of the singular values that we are "thrown" away while making the approximation.

\begin{align*}
||\textbf{X}_{\text{noise}}-\tilde{\textbf{X}}_{\text{noise}, r}||^2_{\text{F}} = \sum_{i=r+1}^n \sigma_i^2
\end{align*}

\noindent{}The low-rank approximation is performed and the results obtained are presented in the table below:

\begin{table}[H]
\tabulinesep=0.9mm
\centering
\begin{tabu}{c|c|}
\cline{2-2}
                                           & Frobenius Norm Squared \\ \hline
\multicolumn{1}{|c|}{$||\textbf{X}-\textbf{X}_{\text{noise}}||_{\text{F}}^2$}             			& 2434.624	\\ \hline
\multicolumn{1}{|c|}{$||\textbf{X}-\tilde{\textbf{X}}_{\text{noise}}||_{\text{F}}^2$} 				& 732.789	\\ \hline
\multicolumn{1}{|c|}{$||\textbf{X}_{\text{noise}}-\tilde{\textbf{X}}_{\text{noise}}||_{\text{F}}^2$}	& 1703.893	\\ \hline
\end{tabu}
\end{table}

\noindent{}Note that the amount of energy that was "thrown" away during the low-rank approximation, 1703.893, is exactly equal to the improvement observed in $||\textbf{X}-\tilde{\textbf{X}}_{\text{noise}}||_{\text{F}}^2$.\\

\noindent{}c. $\hat{\textbf{B}}_{\text{OLS}}$ and $\hat{\textbf{B}}_{\text{PCR}}$ are calculated using the equations (\ref{eq:ols_estimate}) and (\ref{eq:pcr_estimate}). 


\begin{align}
\hat{\textbf{B}}_{\text{OLS}} &= \bigg(\textbf{X}_{\text{noise}}^{\text{T}}\textbf{X}_{\text{noise}}\bigg)^{-1} \textbf{X}_{\text{noise}}^{\text{T}}\textbf{Y} \label{eq:ols_estimate}\\
\hat{\textbf{B}}_{\text{PC}} &= \textbf{V}_{1:r}\bigg(\bm{\Sigma}_{1:r}\bigg)^{-1}\textbf{U}_{1:r}^{\text{T}}\textbf{Y} \label{eq:pcr_estimate}
\end{align} 

\noindent{}Note that it is critical that $\hat{\textbf{B}}_{\text{OLS}}$ is calculated using $\textbf{X}_{\text{noise}}$ and not $\textbf{X}$. Matlab throws a warning if $\textbf{X}$ is used; however this is expected because $\textbf{X}$ is sub-rank. Next, using (\ref{eq:ols}) and (\ref{eq:pcr}), estimation of $\textbf{Y}$ is performed. 


\begin{align}
\hat{\textbf{Y}}_{\text{OLS}}&=\textbf{X}_{\text{noise}}\hat{\textbf{B}}_{\text{OLS}}\label{eq:ols}\\
\hat{\textbf{Y}}_{\text{PCR}}&=\tilde{\textbf{X}}_{\text{noise}}\hat{\textbf{B}}_{\text{PCR}}\label{eq:pcr}
\end{align}

\noindent{}The estimation errors are presented in the table below. From this data alone, it is not clear that Principal Component Regression (PCR) scheme is any better than the Ordinary Least Squares (OLS) scheme. In fact, based purely on the training error obtained, one might conclude that OLS is a better estimation scheme.

\begin{table}[H]
\tabulinesep=0.9mm
\centering
\begin{tabu}{c|c|}
\cline{2-2}
                                           & Euclidean Norm Squared \\ \hline
\multicolumn{1}{|c|}{$||\textbf{Y}-\hat{\textbf{Y}}_{\text{OLS}}||_2^2$}	& 3551		\\ \hline
\multicolumn{1}{|c|}{$||\textbf{Y}-\hat{\textbf{Y}}_{\text{PCR}}||_2^2$}	& 3577		\\ \hline
\end{tabu}
\end{table}

\noindent{}For a fair comparison of the PCR and OLS schemes, the estimates $\hat{\textbf{B}}_{\text{OLS}}$ and $\hat{\textbf{B}}_{\text{PCR}}$ must be evaluated on test data. Evaluation on test data was performed and the results obtained are presented in the table below. The results are telling of the fact that PCR regression has produced a better estimate $\hat{\textbf{B}}_{\text{PCR}}$. While the OLS scheme is able to fit the training data well, when evaluated on test data, the scheme under performs the PCR scheme. 


\begin{table}[H]
\tabulinesep=0.9mm
\centering
\begin{tabu}{c|c|}
\cline{2-2}
                                           & Euclidean Norm Squared \\ \hline
\multicolumn{1}{|c|}{$||\textbf{Y}_{\text{test}}-\hat{\textbf{Y}}_{\text{test-OLS}}||_2^2$}	& 2374		\\ \hline
\multicolumn{1}{|c|}{$||\textbf{Y}_{\text{test}}-\hat{\textbf{Y}}_{\text{test-PCR}}||_2^2$}	& 2354		\\ \hline
\end{tabu}
\end{table}


\noindent{}d. Lastly, the estimates $\hat{\textbf{B}}_{\text{OLS}}$ and $\hat{\textbf{B}}_{\text{PCR}}$ are evaluated over 100 randomly generated testing datasets. The results obtained for presented in the table below:

\begin{table}[H]
\tabulinesep=0.9mm
\centering
\begin{tabu}{c|c|}
\cline{2-2}
                                           & Euclidean Norm Squared \\ \hline
\multicolumn{1}{|c|}{$\mathbb{E}\big\{||\textbf{Y}-\hat{\textbf{Y}}_{\text{OLS}}||_2^2\big\}$}	& 2351		\\ \hline
\multicolumn{1}{|c|}{$\mathbb{E}\big\{||\textbf{Y}-\hat{\textbf{Y}}_{\text{PCR}}||_2^2\big\}$}	& 2328		\\ \hline
\end{tabu}
\end{table}

\noindent{}It is clear that $\hat{\textbf{B}}_{\text{PCR}}$ is a much better estimate of $\textbf{B}$. The results obtained by averaging over 100 randomly generated test datasets corroborate the results obtained in part c. The PCR scheme performs better on test data.

\newpage
\subsection{Real World Signals: Respiratory Sinus Arrhythmia from RR-Intervals}

\noindent{}a. The figure below shows the standard periodogram obtained for each of the three unique sets of RRI data.

\begin{figure}[H]
\centering{}
\includegraphics[width=0.32\textwidth]{part2/eeg_trial_1_standard}
\includegraphics[width=0.32\textwidth]{part2/eeg_trial_2_standard}
\includegraphics[width=0.32\textwidth]{part2/eeg_trial_3_standard}
\caption{Standard Periodograms of RRI Data obtained from Three Separate Trials}
\end{figure}

\noindent{}The averaged periodogram with different window lengths (50 samples, 150 samples, 200 samples) for each unique trial are shown below. Note that the segments used to average the periodogram did not have any overlaps. A Hamming window was applied to each segment. The Hamming window was developed to minimize the height of the maximum (nearest) side lobe. This minimizes the amount of spectral leakage and produces a spectrum that allows for harmonics of the RSA to be identified in the presence of noise.

\begin{figure}[H]
\centering{}
\includegraphics[width=0.32\textwidth]{part2/eeg_trial_1_averaged}
\includegraphics[width=0.32\textwidth]{part2/eeg_trial_2_averaged}
\includegraphics[width=0.32\textwidth]{part2/eeg_trial_3_averaged}
\caption{Averaged Periodograms, with Different Window Lengths, of RRI Data}
\end{figure}

\noindent{}b. Note that the three trials were conducted such that:
\begin{enumerate}
\item Trial 1: Unconstrained Breathing, Typical Ventilation Rate 20-45 Breaths Per Minute (BPM)
\item Trial 2: Fast Breathing, Ventilation Rate 50 BPM
\item Trial 3: Slow Breathing, Ventilation Rate 15 BPM\\
\end{enumerate}

\noindent{}With this in mind, we study the standard periodograms and note that the first harmonics of the RSA for each trial are:
\begin{enumerate}
\item Trial 1: Peak at 0.1641 Hz, which implies a breathing rate of $2 \times 60 \times 0.1641 = 19.69 \approx 20$ BPM
\item Trial 2: Peak at 0.416 Hz, which implies a breathing rate of $2 \times 60 \times 0.416 = 49.92 \approx 50$ BPM
\item Trial 3: Peak at 0.125 Hz, which implies a breathing rate of $2  \times 60 \times 0.125 = 15$ BPM\\
\end{enumerate}

\noindent{}We study the averaged periodograms and find that they produce smoother estimates because increasing the number of samples for which we average the periodogram means that the variance of the estimate decreases. Averaging over 200 samples, the 2nd harmonic in Trial 1 at $f=0.332$ Hz is clear. This harmonic cannot be distinguished in the standard periodogram. In fact, when averaged over 50 and 100 samples, the peak of the 2nd harmonic does not occur at $f=0.332$ Hz; the peaks actually occur at $f=0.2773$ Hz and $f=0.2949$ Hz for the 50 samples and 100 samples periodograms respectively. \textbf{This means that, in an attempt to remove noise, by averaging over smaller segments, we have lost the resolution and the ability to correctly identify the 2nd harmonic.} Similarly, for other trials, harmonics are much more evident in the averaged periodogram with a small shift introduced when averaged over 50 and 100 samples.\\

\noindent{}c. The figure below shows the results obtained when the RRI data is modeled as an AR process. For Trial 1, it is impossible to fit an AR model that models any harmonic beyond the fundamental. This is because of the fact that in the original signal, the only significant periodic component occurs at $f=0.1641$ Hz. As such, increasing the model order amplifies the peak at $f=0.1641$ Hz instead of introducing more peaks. In contrast, the averaged periodogram, when averaged over 200 sample segments, is able to model the harmonics well. In addition, notice that beyond order 10, an artifact starts to occur at 1.5 Hz. This is not present in the averaged periodogram.

\begin{figure}[H]
\centering{}
\includegraphics[width=0.32\textwidth]{part2/ar_spectrum_eeg_trial_1_order_2}
\includegraphics[width=0.32\textwidth]{part2/ar_spectrum_eeg_trial_1_order_4}
\includegraphics[width=0.32\textwidth]{part2/ar_spectrum_eeg_trial_1_order_10}
\includegraphics[width=0.32\textwidth]{part2/ar_spectrum_eeg_trial_2_order_8}
\includegraphics[width=0.32\textwidth]{part2/ar_spectrum_eeg_trial_2_order_12}
\includegraphics[width=0.32\textwidth]{part2/ar_spectrum_eeg_trial_2_order_20}
\includegraphics[width=0.32\textwidth]{part2/ar_spectrum_eeg_trial_3_order_2}
\includegraphics[width=0.32\textwidth]{part2/ar_spectrum_eeg_trial_3_order_13}
\includegraphics[width=0.32\textwidth]{part2/ar_spectrum_eeg_trial_3_order_35}
\caption{Autoregressive Spectrum Estimate for Trial 1,2,3 for Different Model Orders}
\end{figure}

\noindent{}For Trial 2, it is clear that increasing the model order provides greater degrees of freedom that are able to describe the 2nd and possibly 3rd harmonic well. For Trial 3, a similar trend is observed. Using an order 35 model, all the peaks can be accurately modeled. \textbf{For Trial 3, although it is possible to replicate the peaks observed in the standard periodogram, a very high model order is required and this is usually not a good idea as real-world process usually do not have that much memory.}\\

\noindent{}Note that, although the AR model based spectrum has its short-comings described above, in general the estimates are very smooth and only tiny offsets/artifacts are observed.

