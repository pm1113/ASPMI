\section{Adaptive Signal Processing}

\subsection{The Least Mean Square (LMS) Algorithm}

\noindent{}a. The AR(2) process defined in the coursework has the following general form:

\begin{align}
x(n) = a_1 x(n-1) + a_2 x(n-2) + \eta(n)(n) \label{eq:ar_ref}
\end{align}

\noindent{}where $\eta(n) \sim \mathcal{N}(0,\sigma_{\eta}^2)$. Since the correlation matrix $\textbf{R}_x = \mathbb{E}\{\textbf{x}(n)\textbf{x}(n)^T\}$, where $\textbf{x}(n)=[x(n-1), x(n-2)]^T$, the entries of the matrix are:

\begin{align*}
\textbf{R}_x &= 
\begin{bmatrix}
\mathbb{E}\{x(n-1)x(n-1)\} \ \mathbb{E}\{x(n-1)x(n-2)\} \\ 
\mathbb{E}\{x(n-2)x(n-1)\} \ \mathbb{E}\{x(n-2)x(n-2)\}
\end{bmatrix}
\end{align*} 

\noindent{}In order to determine the entries, we multiply (\ref{eq:ar_ref}) with $x(n-k)$ and obtain:

\begin{align*}
x(n)x(n-l) = a_1 x(n-1)x(n-k) + a_2 x(n-2)x(n-l) + \eta(n)x(n-k)
\end{align*}

\noindent{}By taking expectations, we get:

\begin{align*}
\gamma(k) = \mathbb{E}\{x(n)x(n-k)\} =  a_1 \mathbb{E}\{x(n-1)x(n-k)\} + a_2  \mathbb{E}\{x(n-2)x(n-k)\}  + \mathbb{E}\{\eta(n)x(n-k)\}
\end{align*}

\noindent{}The above equation can be simplified and 3 simultaneous equations can be generated to obtain the entries of the correlation matrix:

\begin{align*}
\gamma(0) &= a_1 \gamma(1) + a_2\gamma(2) + \sigma_{\eta}^2 \\
\gamma(1) &= a_1 \gamma(0) + a_2\gamma(1) \\
\gamma(2) &= a_1 \gamma(1) + a_2\gamma(0)
\end{align*}

\noindent{}Solving the simultaneous equations with $a_1=0.1$, $a_2=0.8$ and $\sigma_{\eta}^2=0.25$, we obtain the ACF matrix:

\begin{align*}
\textbf{R}_x &= 
\begin{bmatrix}
\mathbb{E}\{x(n-1)x(n-1)\} \ \mathbb{E}\{x(n-1)x(n-2)\} \\ 
\mathbb{E}\{x(n-2)x(n-1)\} \ \mathbb{E}\{x(n-2)x(n-2)\}
\end{bmatrix}
=
\begin{bmatrix}
\gamma(0) \ \gamma(1) \\ 
\gamma(1) \ \gamma(0)
\end{bmatrix}
= 
\begin{bmatrix}
\frac{25}{27} \ \frac{25}{54} \\ 
\frac{25}{54} \ \frac{25}{27}
\end{bmatrix}
\end{align*} 

\noindent{}To find the range of values for $\mu$ for which the LMS algorithm will convergent in mean, the eigenvalues of $\textbf{R}_x$ has to be found and the following inequality in (\ref{eq:convergence}) has to be satisfied.

\begin{align}
0 \ < \ \mu \ < \ \frac{2}{\lambda_{max}} \label{eq:convergence}
\end{align}

\noindent{}If the inequality is satisfied, the LMS algorithm will converge to the Winer-Hopf solution. The maximum eigenvalue of $\textbf{R}_x$ is 1.3889 and thus as long as $0 \ < \ \mu \ < \ 1.44$, the LMS algorithm will converge in mean.

\begin{figure}[H]
\centering{}
\includegraphics[width=0.32\textwidth]{part3/ar_lms_mu_1_realisation}
\includegraphics[width=0.32\textwidth]{part3/ar_lms_mu_100_realisation}
\caption{Analysis of Effects of Different Step Size on Convergence Speed of LMS Alogirthm}
\end{figure}

\noindent{}b. The LMS adaptive predictor using $N=1000$ samples of $x(n)$ was implemented. Results of 1 realization and the average of 100 realizations are shown in the figure above for $\mu=0.01$ and $\mu=0.05$. Looked at the squared prediction error, it is clear that on average we obtain faster convergence when $\mu=0.05$. This is exactly as expected as a larger value of $\mu$ would allow the algorithm to decent down the error at a faster rate.

\begin{figure}[H]
\centering{}
\includegraphics[width=0.32\textwidth]{part3/time_average_mu_01}
\includegraphics[width=0.32\textwidth]{part3/time_average_mu_05}
\caption{Time-Average of Steady State Error for $\mu=0.01$ and $\mu=0.05$}
\end{figure}


\noindent{}c. The figure above shows the steady-state of the ensemble-average learning curves using 100 independent trials of the experiment. Based on the 100 independent realizations, the excess mean square error, misadjustment and theoretical misadjustment are calculated and the results are shown in the table below. The empirical misadjustment are slightly higher than the theoretical values. Regardless, it is abundantly clear that having a smaller step size introduces a much smaller excess mean square error and results in a smaller misadjustment value. However, as observed above, having a smaller step size causes the rate of convergence to be slow.  

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
mu   & EMSE   & Empirical Misadjustment 	& Theoretical Misadjustment		\\ \hline
0.01 & 0.0032 & 0.0128                 	& 0.0093                   		\\ \hline
0.05 & 0.124  & 0.0496                 	& 0.0463                  		\\ \hline
\end{tabular}
\end{table}

\noindent{}d. The graphs below show how the coefficients evolve over time. For the rest of the section, most graphs will show the evolution of coefficients, averaged over 100 realizations, and thus it is good to get a feel of what the graphs look like for just 1 realization. Real world adaptive filters will more often than not only work with 1 realization of a random signal. Looking more specifically at the evolution of coefficients for $\mu=0.05$ for 1 realization, it is clear that speed has been traded off for steady-state error. The amount of variation around the ideal values is significant.

\begin{figure}[H]
\centering{}
\includegraphics[width=0.32\textwidth]{part3/ar_coefficients_evolution_mu_01_1_trial}
\includegraphics[width=0.32\textwidth]{part3/ar_coefficients_evolution_mu_05_1_trial} \\ 
\includegraphics[width=0.32\textwidth]{part3/ar_coefficients_evolution_mu_01}
\includegraphics[width=0.32\textwidth]{part3/ar_coefficients_evolution_mu_05}
\caption{Evolution of Coefficients for 1 Trial and 100 Trial, for $\mu=0.01$ and $\mu=0.05$}
\label{fig:steady_state_convergence}
\end{figure}

\noindent{}Next, to answer the question directly, we observe Figure \ref{fig:steady_state_AR}. It is clear that for both coefficients, having a small value of $\mu$ results in the steady state value that is much closer to the ideal values of $a_1=0.1$ and $a_2=0.8$. In addition, having a small value of $\mu$ results in a much smaller steady-state variance. However, looking at Figure \ref{fig:steady_state_convergence}, we find that setting $\mu=0.01$, we do not approach the steady state value until the 900th iteration. In contrast, using $\mu=0.05$, the algorithm converges in about 200 iterations.

\begin{figure}[H]
\centering{}
\includegraphics[width=0.32\textwidth]{part3/steady_state_ar_coeff_1}
\includegraphics[width=0.32\textwidth]{part3/steady_state_ar_coeff_2}
\caption{Steady State Values of Coefficients $a_1$ and $a_2$, for $\mu=0.01$ and $\mu=0.05$}
\label{fig:steady_state_AR}
\end{figure}

\noindent{}e. In order to minimize the cost function, we differentiate it and set the derivative to 0. Mathematically, we obtain:

\begin{align}
J(n) &= \frac{1}{2}\Bigg(e^2(n)+\gamma||\textbf{w}(n)||^2\Bigg) \label{eq:cost_func}\\ 
\frac{\partial J}{\partial \textbf{w}} &= e(n)\frac{\partial e}{\partial \textbf{w}} + \gamma \textbf{w}(n) = 0 \nonumber
\end{align}

\noindent{}Since,

\begin{align*}
\frac{\partial e}{\partial \textbf{w}} = \frac{\partial}{\partial \textbf{w}}\Bigg(\textbf{x}(n) - \textbf{w}^T(n)\textbf{x}(n)\Bigg) = -\textbf{x}(n)
\end{align*}

\noindent{}Then,

\begin{align*}
\frac{\partial J}{\partial \textbf{w}} = -\gamma \textbf{w}(n) + e(n)
\end{align*}

\noindent{}Now, using the gradient decent method to reach the optimal value of $w$, we have to implement (\ref{eq:w_update})

\begin{align}
\textbf{w}(n) = \textbf{w}(n) - \mu \nabla_{w}J(n) \label{eq:w_update}
\end{align}

\noindent{}Substituting the derivative of the cost function, we obtain:

\begin{align}
\textbf{w}(n+1) &= \textbf{w}(n) - \mu \Bigg(-\gamma \textbf{w}(n) + e(n)\Bigg)\textbf{x}(n) \nonumber\\
\textbf{w}(n+1) &= (1-\mu\gamma)\textbf{w}(n) - \mu e(n)\textbf{x}(n) \label{eq:leaky_lms}
\end{align}

\noindent{}Thereby showing that the leaky LMS equation shown in the notes and derived in (\ref{eq:leaky_lms}) is equivalent to minimizing the cost function shown in (\ref{eq:cost_func}).\\

\noindent{}f. The graphs shown in Figure \ref{fig:leaky_lms} show evolution of coefficients for different values of $\mu$ and $\gamma$ using the leaky LMS algorithm. The steady-state values do not converge to the true values of $a_1$ and $a_2$. In fact, the larger the value of $\gamma$, the greater the steady-state bias. The Weiner-Hopf solution was briefly mentioned above. It gives the optimal weights calculated using the autocorrelation matrix, $\textbf{R}$, and the cross correlation vector, $\textbf{p}$:

\begin{align*}
\textbf{w}_{opt} = \textbf{R}^{-1}\textbf{p}
\end{align*}

\noindent{}The LMS, under certain bounds on the value of $\mu$, iteratively arrives at the Weiner-Hopf solution. The motivation of the algorithm is to prevent the inverting the autocorrelation matrix. The leaky LMS however does not converge to the Wiener-Hopf solution. The leaky LMS minimizing a cost function that is slightly different than the cost function that the LMS algorithm minimizes and thus it converges to:

\begin{align*}
\lim_{k \rightarrow \infty} \mathbb{E}[\textbf{w}_{k}] = (\textbf{R} + \gamma\textbf{I})^{-1}\textbf{p}
\end{align*}

\noindent{}The additional, $\gamma\textbf{I}$ term causes a bias that means that the leaky LMS does not actually converge to the Wiener-Hopf solution. In fact, since $\textbf{p} = [\gamma(1), \gamma(2)]^T$, we can mathematically determine the values to which the algorithm converges:

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
$\gamma$ & $a_1$     & $a_2$     \\ \hline
0.1   & 0.1542 & 0.3919 \\ \hline
0.5   & 0.1626 & 0.4992 \\ \hline
0.9   & 0.1319 & 0.7076 \\ \hline
\end{tabular}
\end{table}

\noindent{}Notice that the graphs confirm the theoretical results calculated above. The leaky LMS displays the same properties with regards to $\mu$. A smaller $\mu$ means slower convergence but a smaller steady-state error whereas a larger $\mu$ means faster convergence but a larger steady-state error. 

\begin{figure}[H]
\centering{}
\includegraphics[width=0.32\textwidth]{part3/leaky_mu_01_gamma_01}
\includegraphics[width=0.32\textwidth]{part3/leaky_mu_01_gamma_05}
\includegraphics[width=0.32\textwidth]{part3/leaky_mu_01_gamma_09}
\includegraphics[width=0.32\textwidth]{part3/leaky_mu_05_gamma_01}
\includegraphics[width=0.32\textwidth]{part3/leaky_mu_05_gamma_05}
\includegraphics[width=0.32\textwidth]{part3/leaky_mu_05_gamma_09}
\caption{Effect of increasing $\gamma$ on the Steady State Values of Coefficients $a_1$ and $a_2$, for $\mu=0.01$ and $\mu=0.05$}
\label{fig:leaky_lms}
\end{figure}

\subsection{Adaptive Step Sizes}

\noindent{}a. Using the LMS algorithm, we have shown the effect that $\mu$ has on the convergence time as well as the misadjustment. Depending on the application, we can select a $\mu$ to satisfy constraints on convergence time or overshoot or steady-state error. The non-varying nature of $\mu$ makes the LMS algorithm computationally cheap. However, keeping a constant $\mu$ has certain shortcomings. A major shortcoming is that certain choices $\mu$ can cause the algorithm to diverge instead of converging to a solution. \\

\noindent{}The Gradient Adaptive Step Size (GASS) algorithms allow $\mu$ to be time-varying. The algorithms are generally implemented such that the steady-state learning rates approach 0, thereby ensuring stability and convergence. The adaptive nature of the step sizes gives us the flexibilty to vary $\mu$ according to the error that is incurred. The three GASS algorithms specified in the coursework all modify the step size according to the following equation:

\begin{align*}
\mu(n+1)=\mu(n) + \rho e(n) \textbf{x}^T(n)\bm{\psi}(n)
\end{align*}

\noindent{}In all algorithms the value of $\rho$ is a constant while $\phi$ is time-varying. The value of $\phi$ is altered using the following equations:

\begin{align*}
\text{Benveniste}&: \bm{\psi}(n)=[\textbf{I}-\mu(n-1)\textbf{x}(n-1)\textbf{x}^T(n-1)]\bm{\psi}(n-1) + e(n-1)\bm{x}(n-1) \\
\text{Ang \& Farhang}&: \bm{\psi}(n)=\alpha\bm{\psi}(n-1) + e(n-1)\bm{x}(n-1), \ \ \ \ \ 0 < \alpha < 1 \\
\text{Matthew \& Xie}&: \bm{\psi}(n)=e(n-1)\bm{x}(n-1)
\end{align*}

\noindent{}Notice that in the Benveniste algorithm, the update equation for $\bm{\psi}$ is an adaptive low-pass filter with the instantaneous gradient $e(n-1)x(n-1)$ as the input. This algorithm has the ability to deal with noise in the gradient estimate. The Ang \& Farhang algorithm modifies the Benveniste algorithm by turning the update equation for $\bm{\psi}$ from an adaptive to a non-adaptive low-pass filter. The algorithm trades computational complexity for slightly worse convergence properties. The Matthew \& Xie algorithm goes a step further and removes the low-pass filtering that the other two algorithms perform on the instantaneous gradient. This makes the algorithm even faster, however once again convergence properties are sacrificed for speed.\\

\noindent{}The graphs below show the weight error curves for just 1 trial of the algorithms. It is clear that all of the GASS algorithms perform significantly better than the LMS algorithms with a constant step-size.

\begin{figure}[H]
\centering{}
\includegraphics[width=0.32\textwidth]{part3/convergence_of_weights}
\includegraphics[width=0.32\textwidth]{part3/convergence_of_weights_db}
\caption{Comparison of Convergence Time using Adaptive Step Sizes and Standard LMS Algorithms}
\end{figure}

\noindent{}The averaged results of a 100 trials are graphed below. The first thing to notice is that the LMS alogrithms with a fixed step-size do not converge to the correct weights. The results obtained for the GASS algorithms are exactly as expected. The Benveniste algorithm, which is computationally the most expensive, is able to converge the fastest. The next to follow is the Ang \& Farhang algorithm. Notice that although the Mattew \& Xie algorithm performs the worst amongst the GASS algorithms, it is still significantly better than the LMS algorithms without adaptive step-sizes. Notice also that all GASS algorithms do not have an offset.


\begin{figure}[H]
\centering{}
\includegraphics[width=0.32\textwidth]{part3/convergence_of_weights_averaged}
\includegraphics[width=0.32\textwidth]{part3/convergence_of_weights_averaged_db}
\caption{Studying Convergence Time by Averaging Weights over 100 Random Realisations}
\end{figure}


\noindent{}b. The normalised LMS (NLMS) algorithm has the following update equation:

\begin{align}
\textbf{w}(n+1) = \textbf{w}(n) + \frac{\beta}{\epsilon + ||\textbf{x}||^2} e(n)\textbf{x}(n)\label{eq:nlms_algo}
\end{align}

\noindent{}To show that the update equation in (\ref{eq:nlms_update}) based upon the \textit{a posteriori} error $e_p(n)=d(n)-\textbf{x}^T(n)\textbf{w}(n+1)$ is equivalent to the NLMS algorithm, we first multiply both sides by $\textbf{x}^T(n)$. 

\begin{align}
\textbf{w}(n+1) &= \textbf{w}(n) + \mu e_p(n)\textbf{x}(n) \label{eq:nlms_update}\\
\textbf{x}^T(n)\textbf{w}(n+1) &= \textbf{x}^T\textbf{w}(n) + \textbf{x}^T \mu e_p(n) \textbf{x}(n) \nonumber
\end{align}

\noindent{}Next, we add $\textbf{d}(n)$ to both sides and rearrange:

\begin{align}
\textbf{d}(n)+\textbf{x}^T(n)\textbf{w}(n+1) &= \textbf{d}(n)+\textbf{x}^T\textbf{w}(n) + \textbf{x}^T \mu e_p(n) \textbf{x}(n) \nonumber\\
\textbf{d}(n)-\textbf{x}^T\textbf{w}(n) &= \textbf{d}(n)-\textbf{x}^T(n)\textbf{w}(n+1) + \textbf{x}^T \mu e_p(n) \textbf{x}(n)\label{eq:nlms_mid_proof}
\end{align}

\noindent{}Next we substitute the \textit{a posteriori} error into (\ref{eq:nlms_mid_proof}):

\begin{align}
e(n) &= e_p(n) + \textbf{x}^T(n) \mu e_p(n) x(n) \nonumber\\
e(n) &= e_p(n) \bigg(1+\mu ||\textbf{x}||^2\bigg) \nonumber\\
e_p(n) &= \frac{e(n)}{1+\mu||\textbf{x}||^2} \label{eq:a_posteriori_error}
\end{align}

\noindent{}We can now substitute (\ref{eq:a_posteriori_error}) into the original update equation (\ref{eq:nlms_update}) and obtain the NLMS algorithm defined in (\ref{eq:nlms_algo}):

\begin{align}
\textbf{w}(n+1) &= \textbf{w}(n) + \mu \frac{e(n)}{1+\mu ||\textbf{\textbf{x}}||^2}\textbf{x}(n) \nonumber\\
&= \textbf{w}(n) + \frac{1}{\frac{1}{\mu}+ ||\textbf{\textbf{x}}||^2}e(n)\textbf{x}(n) 
\end{align}

\noindent{}Thereby proving that the update equation in (\ref{eq:nlms_update}) based the \textit{a posteriori} error with $\epsilon=\frac{1}{\mu}$ and $\beta=1$ is equivalent to the NLMS algorithm defined in (\ref{eq:nlms_algo}). Since $\epsilon$ is inversely proportional to the step size $\mu$, the algorithm does not become unstable even if $||x||^2$ is extremely small.

\begin{figure}[H]
\centering{}
\includegraphics[width=0.32\textwidth]{part3/compare_benveniste_gngd}
\caption{Comparing Convergence Speed of GNGD and Benveniste's GASS Algorithms}
\end{figure}

\noindent{}c. From the figure above, it is clear that the Generalized Normalized Gradient Descent (GNGD) algorithm converges much faster than the Benveniste algorithm. Both algorithms are similar in the fact that they modify the learning rate in an adaptive manner. The difference lies in the fact that the GNGD algorithm modifies the learning rate in a non-linear manner. The non-linear updates provide compensation for the assumptions made in the derivation of the NLMS. The non-linear updates make the algorithm more robust and the increased stability make it effective at processing non-linear and non-stationary signals (REFERENCE MANDIC PAPER).\\

\noindent{}In terms of the computational complexity of both algorithms, the GNGD algorithm also beats the Benveniste algorithm. The number of additions and multiplications for the Benveniste algorithm are presented in the Table \ref{tab:ben}. The variable $M$ represents the model order. It is clear that the algorithm scales quadratically with the model order. The computational complexity of the algorithm can be attributed to the outter product $\textbf{x}(n-1)\textbf{x}^T(n-1)$ that has to performed to update the value of $\bm{\psi}(n)$. 

\begin{table}[H]
\tabulinesep=0.9mm
\centering
\begin{tabu}{|c|c|c|c|}
\hline
Step                   			& Sub-Step & Multiplications & Additions \\ \hline
\multirow{6}{*}{$\bm{\psi}(n)$} 	& $\textbf{x}(n-1)\textbf{x}^T(n-1)$ 													&$M^2$         		&0\\ \cline{2-4} 
                       			& $\mu(n-1)\textbf{x}(n-1)\textbf{x}^T(n-1)$       										&$M^2$       		&0\\ \cline{2-4} 
                       			& $I-\mu(n-1)\textbf{x}(n-1)\textbf{x}^T(n-1)$    										&0        			&$M$\\ \cline{2-4} 
                       			& $[I-\mu(n-1)\textbf{x}(n-1)\textbf{x}^T(n-1)]\bm{\psi}(n-1)$       						&$M^2$ 				&$M^2-M$	\\ \cline{2-4} 
                       			& $e(n-1)\textbf{x}(n-1)$       															&$M$        		&0\\ \cline{2-4} 
                       			& $[I-\mu(n-1)\textbf{x}(n-1)\textbf{x}^T(n-1)]\bm{\psi}(n-1) + e(n-1)\textbf{x}(n-1)$  	&0          			&$M$\\ \hline 
\multirow{4}{*}{$\mu(n+1)$} 		& $\textbf{x}^T(n)\bm{\psi}(n)$ 															&$M$         		&$M-1$\\ \cline{2-4}
								& $\rho e(n)$       																		&0        			&$1$\\ \cline{2-4} 
                       			& $\rho e(n) \textbf{x}^T(n)\bm{\psi}(n)$    											&$0$        			&$1$\\ \cline{2-4} 
                       			& $\mu(n)+\rho e(n) \textbf{x}^T(n)\bm{\psi}(n)$       									&$0$ 				&$1$	\\ \hline
\multirow{3}{*}{$\textbf{w}(n+1)$}& $\mu(n)e(n)$ 																			&$0$         		&$1$\\ \cline{2-4}
								& $\mu e(n)\textbf{x}(n)$       															&$M$        			&$0$\\ \cline{2-4} 
                       			& $\textbf{w}(n) + \mu e(n)\textbf{x}(n)$    											&$0$        			&$M$\\ \hline
\multicolumn{1}{l}{}                         & \multicolumn{1}{l}{}          & \multicolumn{1}{l}{}                 & \multicolumn{1}{l}{}           \\ \hline
\multicolumn{2}{|c|}{Total}    	&\multicolumn{1}{c|}{$\mathcal{O}(M^2)$} & \multicolumn{1}{c|}{$\mathcal{O}(M^2)$} \\ \hline  
\end{tabu}
\caption{Computational Complexity of the Benveniste Algorithm}
\label{tab:ben}
\end{table}

\noindent{}Table \ref{tab:gngd} shows the computational complexity of the GNGD algorithm. It is clear that the algorithm only scales linearly with the model order. 

\begin{table}[H]
\tabulinesep=0.9mm
\centering
\begin{tabu}{|c|c|c|c|}
\hline
Step                   			& Sub-Step & Multiplications & Additions \\ \hline
\multirow{7}{*}{$\epsilon(n+1)$}	& $\textbf{x}^T(n)\textbf{x}(n-1)$ 																			&$M$         		&$M-1$\\ \cline{2-4} 
                       			& $e(n)e(n-1)$       																						&$1$       			&0\\ \cline{2-4} 
                       			& $e(n)e(n-1)\textbf{x}^T(n)\textbf{x}(n-1)$    																&$1$        			&$0$\\ \cline{2-4} 
                       			& $||\textbf{x}(n-1)||^2$       																				&$M$ 				&$M-1$	\\ \cline{2-4} 
                       			& $(\epsilon(n-1)+||\textbf{x}(n-1)||^2)^2$       															&$1$        			&1\\ \cline{2-4} 
                       			& $\rho\mu\frac{e(n)e(n-1)\textbf{x}^T(n)\textbf{x}(n-1)}{(\epsilon(n-1)+||\textbf{x}(n-1)||^2)^2}$  			&$3$          		&$0$\\  \cline{2-4}                      			
                       			& $\epsilon(n)+\rho\mu\frac{e(n)e(n-1)\textbf{x}^T(n)\textbf{x}(n-1)}{(\epsilon(n-1)+||\textbf{x}(n-1)||^2)^2}$	&$0$        			&1\\ \hline 
\multirow{5}{*}{$\textbf{w}(n+1)$}& $||\textbf{x}(n)||^2$ 																					&$M$         		&$M-1$\\ \cline{2-4} 
                       			& $\frac{\beta}{\epsilon(n) + ||\textbf{x}(n)||^2}$       													&$1$       			&1\\ \cline{2-4} 
                       			& $\frac{\beta}{\epsilon(n) + ||\textbf{x}(n)||^2}e(n)$    													&$1$        			&$0$\\ \cline{2-4} 
                       			& $\frac{\beta}{\epsilon(n) + ||\textbf{x}(n)||^2}e(n)\textbf{x}(n)$       									&$M$ 				&$M-1$	\\ \cline{2-4} 
                       			& $\textbf{w}(n)+\frac{\beta}{\epsilon(n) + ||\textbf{x}(n)||^2}e(n)\textbf{x}(n)$       						&$0$        			&$M$\\ \hline
\multicolumn{1}{l}{}                         & \multicolumn{1}{l}{}          & \multicolumn{1}{l}{}                 & \multicolumn{1}{l}{}           \\ \hline
\multicolumn{2}{|c|}{Total}    	&\multicolumn{1}{c|}{$\mathcal{O}(M)$} & \multicolumn{1}{c|}{$\mathcal{O}(M)$} \\ \hline 
\end{tabu}
\caption{Computational Complexity of the GNGD Algorithm}
\label{tab:gngd}
\end{table}


\subsection{Adaptive Noise Cancellation}

\noindent{}a. By choosing an appropriate value of $\Delta$, the Adaptive Line Enhancement scheme can be set up such that the noise, $\eta(n)$, in the signal, $s(n)$, and the predicted input, $u(n)$ are uncorrelated. If the noise $\eta(n)$ and the predicted input $u(n)$ are uncorrelated, the noise can be suppressed and a clean signal can be obtained. To find an optimal value of $\Delta$, we start with the mean-squared error:

\begin{align*}
\mathbb{E}\bigg\{\bigg(s(n)-\hat{x}(n)\bigg)^2\bigg\} &= \mathbb{E}\bigg\{\bigg(x(n)+\eta(n)-\hat{x}(n)\bigg)^2\bigg\} \\
&=\mathbb{E}\{\eta^2(n)\} + \mathbb{E}\bigg\{\bigg(x(n)-\hat{x}(n)\bigg)^2\bigg\} + 2\mathbb{E}\bigg\{\eta(n)\bigg(x(n)-\hat{x}(n)\bigg)\bigg\}
\end{align*}

\noindent{}Expanding the mean-sqaured error shows that there are three terms that are contributing to the error. Ideally, we would like the mean-squared error to be exactly equal to the noise-power, however by changing the value of $\delta$, it is only possible to manipulate $2\mathbb{E}\bigg\{\eta(n)\bigg(x(n)-\hat{x}(n)\bigg)\bigg\}$. As such, $\delta$ should be chosen so as to minimise $2\mathbb{E}\bigg\{\eta(n)\bigg(x(n)-\hat{x}(n)\bigg)\bigg\}$. It is important to note that since $x(n)$ and $\eta(n)$ are uncorrelated, the minimization problem reduces to minimizing $\mathbb{E}\{\eta(n)\hat{x}(n)\}$. Since $\eta(n)=v(n) + 0.5v(n-2)$ we get:

\begin{align*}
\mathbb{E}\{\eta(n)\hat{x}(n)\} &= \mathbb{E}\bigg\{\bigg( v(n) + 0.5v(n-2)\bigg) \textbf{w}^T \textbf{u}(n)\bigg\} \\
&= \mathbb{E}\bigg\{\bigg( v(n) + 0.5v(n-2)\bigg) \bigg(\sum_{i=0}^{M}w_{i}s(n-\Delta-i)\bigg)\bigg\} \\
&= \mathbb{E}\bigg\{\bigg( v(n) + 0.5v(n-2)\bigg) \bigg(\sum_{i=0}^{M}w_{i}\big(x(n-\Delta-i)+\eta(n-\Delta-i)\big)\bigg)\bigg\} 
\end{align*}

\noindent{}Again using the fact that $x(n)$ and $v(n)$ are uncorrelated:

\begin{align*}
\mathbb{E}\{\eta(n)\hat{x}(n)\} &= \mathbb{E}\bigg\{\bigg( v(n) + 0.5v(n-2)\bigg) \bigg(\sum_{i=0}^{M}w_{i}\eta(n-\Delta-i)\bigg)\bigg\} \\
&= \mathbb{E}\bigg\{\bigg( v(n) + 0.5v(n-2)\bigg) \bigg(\sum_{i=0}^{M}w_{i}\big(v(n-\Delta-i)+0.5v(n-\Delta-i-2)\big)\bigg)\bigg\}
\end{align*}

\noindent{}Thus, it is clear that for a value of $\Delta$ greater than 2, the expectation would reduce to adding the expectations of white noise samples at different time indexes, which are uncorrelated. This is also clear by observing the results presented in Figure \ref{fig:ale}. Note that the top row of the figure shows the averaged results from 100 realisations, whereas the bottom row shows the overlay of 100 realisations, along with the ideal sinewave. All the figures conclusively prove that for $\Delta>2$ the results obtained are significantly better. This is also corroborated by the large decrease in the mean-squared error from when $\Delta$ is increased from $2$ to $3$.

\begin{figure}[H]
\centering{}
\includegraphics[width=0.24\textwidth]{part3/delay_1}
\includegraphics[width=0.24\textwidth]{part3/delay_2}
\includegraphics[width=0.24\textwidth]{part3/delay_3}
\includegraphics[width=0.24\textwidth]{part3/delay_4}
\includegraphics[width=0.24\textwidth]{part3/mpse_delay_1}
\includegraphics[width=0.24\textwidth]{part3/mpse_delay_2}
\includegraphics[width=0.24\textwidth]{part3/mpse_delay_3}
\includegraphics[width=0.24\textwidth]{part3/mpse_delay_4}
\caption{Determining Ideal Value for $\Delta$ to be used in the Adaptive Line Enhancement Algorithm}
\label{fig:ale}
\end{figure}


\noindent{}b. The results in Figure \ref{fig:ale_delay} futher verify the results above. Choosing a value of $\Delta$ greater than $2$ causes the mean-squared error to decrease for all model-orders. Note that arbitarily increase the value of $\Delta$ is however not wise. Note that one of the terms which contributing to $\mathbb{E}\bigg\{\bigg(s(n)-\hat{x}(n)\bigg)^2\bigg\}$ is $\mathbb{E}\bigg\{\bigg(x(n)-\hat{x}(n)\bigg)^2\bigg\}$. Increasing the value of $\Delta$ will increase the delay between $x(n)$ and $\hat{x}(n)$. This effect is also visualized in Figure \ref{fig:ale_delay}. Notice that the output sine-wave is shifted relatively to the ideal input sinewave when $\Delta=25$. This shift causes $\mathbb{E}\bigg\{\bigg(x(n)-\hat{x}(n)\bigg)^2\bigg\}$ to be large, thereby increasing the mean-squared error.


\begin{figure}[H]
\centering{}
\includegraphics[width=0.32\textwidth]{part3/delay_vs_mpse}
\includegraphics[width=0.32\textwidth]{part3/mpse_delay_5}
\includegraphics[width=0.32\textwidth]{part3/mpse_delay_25}
\caption{Studying the Effects of Increasing Delay on the MPSE}
\label{fig:ale_delay}
\end{figure}


\noindent{}Theoretically, a higher model-order should be able to describe the highly periodic process with greater accuracy. However, since we are deriving the solution numerically, factors such as convergence and stability should also be taken into account. Note that for high-model orders, if the mu is not correctly chosen, the mean-squared error starts to increase. It is possible to keep $\mu$ small and increase the model order however it would cause the algorithm to converge slowly. Looking at Figure \ref{fig:ale_model}, a pragmatic choice would be to set $M=6$ and set $\mu=0.005$.

\begin{figure}[H]
\centering{}
\includegraphics[width=0.32\textwidth]{part3/model_order_mpse}
\caption{Studying the Effects of Increasing Model Order on the MPSE}
\label{fig:ale_model}
\end{figure}

\noindent{}c. The performance of the Adaptive Line Enhancement (ALE) algorithm and the Adaptive Noise Cancellation (ANC) algorithm are compared in the figure below. It is clear that the ANC algorithm performs significantly better, once it has converged. However, notice that for small values of $n$, the ANC algorithm incurs errors larger than the ALE algorithm
\begin{figure}[H]
\centering{}
\includegraphics[width=0.32\textwidth]{part3/anc_ale_comparison}
\includegraphics[width=0.32\textwidth]{part3/ale_mpse}
\includegraphics[width=0.32\textwidth]{part3/anc_mpse}
\caption{Comparison of ALE and ANC Configurations for Denoising Sinewave}
\end{figure}

\noindent{}d. The spectrogram below is graphed for reference. It shows the original EEG data with a very strong component at 50 Hz. The length of each segment is $4096$, with an $80\%$ overlap between segments. Also, each segment is windowed with a Hamming window. 

\begin{figure}[H]
\centering{}
\includegraphics[width=0.32\textwidth]{part3/original_POz_spectrogram}
\caption{Original EEG Data collected from POz Location on the Scalp with Strong Component at 50 Hz}
\end{figure}

\noindent{}The first step is to determine the value of $M$ that will remove the strong 50 Hz component. The figure below shows the effect of increasing the model order. Increasing the model order results in a filter that is more efficient at removing the noise. However, notice that certain artifacts have started to occur. With $M=25$ and $\mu=0.01$, the filter is not only removing the 50 Hz component but it is also suppressing components between 40 Hz and 60 Hz.

\begin{figure}[H]
\centering{}
\includegraphics[width=0.24\textwidth]{part3/POz_mu_01_M_10}
\includegraphics[width=0.24\textwidth]{part3/POz_mu_01_M_15}
\includegraphics[width=0.24\textwidth]{part3/POz_mu_01_M_20}
\includegraphics[width=0.24\textwidth]{part3/POz_mu_01_M_25}
\caption{Effect of Increasing Model Order on the Spectrogram of EEG Data}
\end{figure}

\noindent{}To combat the large spread of frequencies over which the filter is acting, we vary $\mu$. The figure below shows the effects of decreasing $\mu$. Decreasing $\mu$ reduces the bias in the ANC algorithm and leads to better performance. It is clear that the filter is now acting over a much smaller range of frequencies. 

\begin{figure}[H]
\centering{}
\includegraphics[width=0.24\textwidth]{part3/POz_mu_025_M_25}
\includegraphics[width=0.24\textwidth]{part3/POz_mu_01_M_25}
\includegraphics[width=0.24\textwidth]{part3/POz_mu_005_M_25}
\includegraphics[width=0.24\textwidth]{part3/POz_mu_001_M_25}
\caption{Effect of Varying $\mu$ on the artifacts observed in the Spectrogram of Denoised EEG Data}
\end{figure}


\noindent{}The periodogram and squared error plots corroborate the findings above. For large values of $\mu$ such as 0.025, large errors are incurred even at low frequencies. In fact, with $\mu=0.025$, the filter is suppressing the DC component to a greater extent than the component at 50 Hz. This does not occur with $\mu=0.001$.

\begin{figure}[H]
\centering{}
\includegraphics[width=0.32\textwidth]{part3/bartlett_cleaned_Poz}
\includegraphics[width=0.32\textwidth]{part3/squared_error_periodogram}
\includegraphics[width=0.32\textwidth]{part3/squared_error_periodogram_bad}
\caption{Welch Periodogram, averaged over 2 second intervals, and Squared Errors}
\end{figure}
